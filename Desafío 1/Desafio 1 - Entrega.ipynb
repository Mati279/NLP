{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec77cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
    "# en sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05748be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60412f9d",
   "metadata": {},
   "source": [
    "### Punto 1\n",
    "\n",
    "Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
    "la similaridad según el contenido del texto y la etiqueta de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e992d4c",
   "metadata": {},
   "source": [
    "Primero vectorizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfea510",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c897d",
   "metadata": {},
   "source": [
    "Sorteamos 5 documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a871dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos 5 documentos al azar del conjunto de entrenamiento\n",
    "docs = np.random.choice(X_train.shape[0], size=5, replace=False)\n",
    "\n",
    "# Guardamos cada doc en su variable\n",
    "doc1 = newsgroups_train.data[docs[0]]\n",
    "doc2 = newsgroups_train.data[docs[1]]\n",
    "doc3 = newsgroups_train.data[docs[2]]\n",
    "doc4 = newsgroups_train.data[docs[3]]\n",
    "doc5 = newsgroups_train.data[docs[4]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fdc31",
   "metadata": {},
   "source": [
    "Para imprimir los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ff8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 1:\n",
      " VESA local bus motherboard,\n",
      "4MB RAM,\n",
      "64K cache,\n",
      "1.2 & 1.44 Floppy,\n",
      "130 MB Hard Drive,\n",
      "IDE controller (2HD&2FD)\n",
      "2S/1P/1G\n",
      "Local Bus 1MB SVGA Video Card,\n",
      "14\" SVGA Monitor (.28dpi)\n",
      "Mini Tower, 101-key Keyboard ...\n",
      "\n",
      "Documento 2:\n",
      " I have a BACK MACHINE and have had one since January.  While I have not \n",
      "found it to be a panacea for my back pain, I think it has helped somewhat. \n",
      "It MAINLY acts to stretch muscles in the back and prevent spasms associated\n",
      "with pain.  I am taking less pain medication than I was previously.  \n",
      "   Th ...\n",
      "\n",
      "Documento 3:\n",
      " I HATE long postings, but this turned out to be rather lengthy....\n",
      "\n",
      "\n",
      "Overall Crime rate:\n",
      "It fell....just like that...\n",
      "\n",
      "Acquiring weapons in Norway:\n",
      "You can buy (almost) all kinds of weapons in Norway, BUT you must have a \n",
      "permit, and a good reason to get the permit....\n",
      "If I would like to have a hand ...\n",
      "\n",
      "Documento 4:\n",
      " \n",
      "Well I can buy a bigger and more powerful server machine because of the \n",
      "significant drop in price year after year.  The link I want to use \n",
      "though (ISDN 64K) is costly and the bandwidth limited.  That's why my\n",
      "interest lies in seeing if such a link can be used and see what traffic \n",
      "goes through it ...\n",
      "\n",
      "Documento 5:\n",
      " \n",
      "\n",
      "Assuming you are presenting it accurately, I don't see how this argument\n",
      "really leads to any firm conclusion.  The material in John (I'm not sure\n",
      "exactly what is referred to here, but I'll take for granted the similarity\n",
      "to the Matt./Luke \"Q\" material) IS different; hence, one could have almost\n",
      "an ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Documento 1:\\n\", doc1[:300], \"...\\n\")\n",
    "print(\"Documento 2:\\n\", doc2[:300], \"...\\n\")\n",
    "print(\"Documento 3:\\n\", doc3[:300], \"...\\n\")\n",
    "print(\"Documento 4:\\n\", doc4[:300], \"...\\n\")\n",
    "print(\"Documento 5:\\n\", doc5[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862ccc4",
   "metadata": {},
   "source": [
    "Medimos la similaridad con el resto de los documentos y vemos los 5 más similares para cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e4b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documento índice: 1860\n",
      "Clase del documento:\n",
      "misc.forsale\n",
      "\n",
      "Los 5 documentos más similares son:\n",
      "- comp.sys.ibm.pc.hardware  |  After a rough start purchasing a 486 system (see earlier post), I'm trying\n",
      "- misc.forsale  |  From: sam.halperin@cccbbs.uceng\n",
      "- comp.sys.ibm.pc.hardware  |  I have a 486DX 25mhz with local bus.  Would I see much of an increase in\n",
      "- comp.sys.ibm.pc.hardware  |  \n",
      "- comp.graphics  |  \n",
      "\n",
      "Documento índice: 7402\n",
      "Clase del documento:\n",
      "sci.med\n",
      "\n",
      "Los 5 documentos más similares son:\n",
      "- sci.med  |  I noticed several years ago that when I took analgesics fairly regularly,\n",
      "- sci.med  |  presented\n",
      "- sci.med  |  I need advice with a situation which occurred between me and a physican\n",
      "- sci.med  |  As promised, below is a personal critique of a Pressure Point Massager \n",
      "- sci.med  |  I am not sure if this is the proper group to post this to but here goes anyway.\n",
      "\n",
      "Documento índice: 1475\n",
      "Clase del documento:\n",
      "talk.politics.guns\n",
      "\n",
      "Los 5 documentos más similares son:\n",
      "- talk.politics.guns  |  Gun clubs:\n",
      "- talk.politics.guns  |  \n",
      "- talk.politics.guns  |  \n",
      "- talk.politics.guns  |  \n",
      "- talk.politics.guns  |  \n",
      "\n",
      "Documento índice: 5433\n",
      "Clase del documento:\n",
      "comp.windows.x\n",
      "\n",
      "Los 5 documentos más similares son:\n",
      "- comp.windows.x  |  \n",
      "- sci.crypt  |  Archive-name: net-privacy/part1\n",
      "- comp.windows.x  |  Archive-name: x-faq/speedups\n",
      "- comp.windows.x  |  Archive-name: x-faq/part1\n",
      "- comp.windows.x  |  \n",
      "\n",
      "Documento índice: 2710\n",
      "Clase del documento:\n",
      "alt.atheism\n",
      "\n",
      "Los 5 documentos más similares son:\n",
      "- alt.atheism  |   \n",
      "- alt.atheism  |  The recent rise of nostalgia in this group, combined with the\n",
      "- alt.atheism  |   \n",
      "- alt.atheism  |  Archive-name: atheism/faq\n",
      "- sci.crypt  |  \n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"\\nDocumento índice:\", doc)\n",
    "    print(\"Clase del documento:\")\n",
    "    print(newsgroups_train.target_names[newsgroups_train.target[doc]])\n",
    "    \n",
    "    cossim = cosine_similarity(X_train[doc], X_train)[0]\n",
    "    mostsim = np.argsort(cossim)[::-1][1:6]\n",
    "    \n",
    "    print(\"\\nLos 5 documentos más similares son:\")\n",
    "    for i in mostsim:\n",
    "        primera_linea = newsgroups_train.data[i].split(\"\\n\")[0]\n",
    "        print(\"-\", newsgroups_train.target_names[newsgroups_train.target[i]], \" | \", primera_linea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d5661",
   "metadata": {},
   "source": [
    "Al menos por las clases de los documentos se puede ver que parece que el algoritmo está funcionando bien ya que las mismas, entre documentos que se suponen similares, parecen coincidir bastante. Ahora vamos a imprimir los primeros 250 caracteres de uno de estos documentos al azar y de sus 5 más similares para ver si la similitud se puede corroborar en la lectura de esas secciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a051e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento índice: 5433\n",
      "Clase: comp.windows.x\n",
      "\n",
      "Texto (primeros 250 caracteres):\n",
      "\n",
      "Well I can buy a bigger and more powerful server machine because of the \n",
      "significant drop in price year after year.  The link I want to use \n",
      "though (ISDN 64K) is costly and the bandwidth limited.  That's why my\n",
      "interest lies in seeing if such a link ...\n",
      "\n",
      "Documentos más similares:\n",
      "\n",
      "-------------------Documento 1:----------------------\n",
      "Clase: comp.windows.x\n",
      "\n",
      "What sort of traffic is generated with the X-calls?  I am curious to find\n",
      "out the required bandwidth that a link must have  if one machine running\n",
      "DV/X is supporting multiple users (clients) and we require adequate response\n",
      "time.  Anyone have any id ...\n",
      "\n",
      "-------------------Documento 2:----------------------\n",
      "Clase: sci.crypt\n",
      "Archive-name: net-privacy/part1\n",
      "Last-modified: 1993/3/3\n",
      "Version: 2.1\n",
      "\n",
      "\n",
      "IDENTITY, PRIVACY, and ANONYMITY on the INTERNET\n",
      "================================================\n",
      "\n",
      "(c) 1993 L. Detweiler.  Not for commercial use except by permission\n",
      "from author, ...\n",
      "\n",
      "-------------------Documento 3:----------------------\n",
      "Clase: comp.windows.x\n",
      "Archive-name: x-faq/speedups\n",
      "Last-modified: 1993/4/15\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\tHOW TO MAXIMIZE THE PERFORMANCE OF X -- monthly posting\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  ...\n",
      "\n",
      "-------------------Documento 4:----------------------\n",
      "Clase: comp.windows.x\n",
      "Archive-name: x-faq/part1\n",
      "Last-modified: 1993/04/04\n",
      "\n",
      "This article and several following contain the answers to some Frequently Asked \n",
      "Questions (FAQ) often seen in comp.windows.x. It is posted to help reduce \n",
      "volume in this newsgroup and to provide h ...\n",
      "\n",
      "-------------------Documento 5:----------------------\n",
      "Clase: comp.windows.x\n",
      "\n",
      "Actually, info@qdeck.com is our customer service department. If you have\n",
      "technical questions, you can write to support@qdeck.com.\n",
      "\n",
      "\n",
      "I expect the limiting factor will be your server machine, not the network\n",
      "itself. To give you a real-world example, h ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elegimos un documento al azar de los ya sorteados\n",
    "doc = np.random.choice(docs, size=1)[0]\n",
    "\n",
    "print(\"Documento índice:\", doc)\n",
    "print(\"Clase:\", newsgroups_train.target_names[newsgroups_train.target[doc]])\n",
    "print(\"\\nTexto (primeros 250 caracteres):\")\n",
    "print(newsgroups_train.data[doc][:250], \"...\\n\")\n",
    "\n",
    "# Calculamos la similaridad coseno con todos los documentos de train\n",
    "cossim = cosine_similarity(X_train[doc], X_train)[0]\n",
    "mostsim = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(\"Documentos más similares:\\n\")\n",
    "t = 1\n",
    "for i in mostsim:\n",
    "    print(f\"-------------------Documento {t}:----------------------\")\n",
    "    t = t + 1\n",
    "    print(\"Clase:\", newsgroups_train.target_names[newsgroups_train.target[i]])\n",
    "    print(newsgroups_train.data[i][:250], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16e11a",
   "metadata": {},
   "source": [
    "Luego de repetir la prueba varias veces se ve que la lectura de los textos (primeros 250 caracteres) confirma la similitud entre los documentos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95d09b",
   "metadata": {},
   "source": [
    "### Punto 2\n",
    "\n",
    "Construir un modelo de clasificación por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79a25db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 en test: 0.5049911553681621\n"
     ]
    }
   ],
   "source": [
    "# Vectorizamos el conjunto de test con el mismo vectorizador\n",
    "X_test = tfidfvect.transform(newsgroups_test.data)\n",
    "\n",
    "# Para cada documento de test buscamos el documento de train más similar\n",
    "y_pred = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    cossim = cosine_similarity(X_test[i], X_train)[0]\n",
    "    doc_max = np.argmax(cossim)      # El más parecido\n",
    "    pred_label = newsgroups_train.target[doc_max]\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "# Pasamos a array de numpy antes de evaular \n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"Macro F1 en test:\", f1_score(newsgroups_test.target, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645dd59",
   "metadata": {},
   "source": [
    "El clasificador por prototipos con similaridad coseno logró un F1 macro de 0.50, esto muestra que es posible capturar bastante bien la estructura temática de los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6cb92",
   "metadata": {},
   "source": [
    "### Punto 3\n",
    "\n",
    "Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
    "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
    "y ComplementNB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bb4fb",
   "metadata": {},
   "source": [
    "Para este punto decidimos primero probar con el mismo vectorizador ya instanciado antes y luego retocar los parámetros para ver si mejora el resultado final. \n",
    "\n",
    "A continuación vamos a crear los dos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c5111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro con MultinomialNB: 0.5854345727938506\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naïve Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "x_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"F1 macro con MultinomialNB:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a0c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro con ComplementNB: 0.692953349950875\n"
     ]
    }
   ],
   "source": [
    "#Complement Naïve Bayes\n",
    "clf = ComplementNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "x_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"F1 macro con ComplementNB:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b5119",
   "metadata": {},
   "source": [
    "Sin haber todavía experimentado con los parámetros del vectorizador pordemos ver que el F1 de ambos modelos es bastente mejor (sobre todo en el CNB) que el del modelo de clasificación por prototipos. \n",
    "\n",
    "Veremos ahora qué pasa si usamos stop words, si usamos unigramas y bigramas y si excluimos las palabras demasiado frecuentes y muy poco frecuentes en los documentos. \n",
    "\n",
    "Para eso volveremos a instanciar el vectorizador utilizando algunos de sus parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96708e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidfvect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "\n",
    "# Vectorizamos de nuevo\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "x_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcb82d",
   "metadata": {},
   "source": [
    "Volvemos a probar los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa603ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro con MultinomialNB: 0.6512187819033597\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naïve Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"F1 macro con MultinomialNB:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c52987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro con ComplementNB: 0.6998859814364937\n"
     ]
    }
   ],
   "source": [
    "# Complement Naïve Bayes \n",
    "clf = ComplementNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"F1 macro con ComplementNB:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf8de6",
   "metadata": {},
   "source": [
    "Conclusiones: \n",
    "\n",
    "- Con el vectorizador por defecto, MultinomialNB logró un F1 macro de 0.58 y CNB de 0.69.  \n",
    "- Ajustando parámetros del vectorizador (stopwords, n-grams y umbrales de frecuencia), el rendimiento mejoró: MultinomialNB alcanzó 0.65 y ComplementNB 0.70.  \n",
    "- Esto muestra que la calidad de la vectorización es importante para el resultado. ComplementNB sigue siendo el mejor aunque prácticamente no se vió beneficiado del nuevo vectorizador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6e49d",
   "metadata": {},
   "source": [
    "### Punto 4\n",
    "\n",
    "Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
    "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f81d81",
   "metadata": {},
   "source": [
    "Antes de continuar con este punto volvemos al vectorizador inicial para evitar que nos muestre bigramas en lugar de palabras sueltas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b806a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "\n",
    "# Vectorizamos de nuevo\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "x_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2accf538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra base: congres\n",
      "Palabras más similares:\n",
      "- actionm\n",
      "- deicisons\n",
      "- propogranda\n",
      "- delcaration\n",
      "- discusing\n",
      "\n",
      "Palabra base: neighborhood\n",
      "Palabras más similares:\n",
      "- pol\n",
      "- demise\n",
      "- alchohol\n",
      "- estimates\n",
      "- pot\n",
      "\n",
      "Palabra base: mars\n",
      "Palabras más similares:\n",
      "- observer\n",
      "- agian\n",
      "- exobiologists\n",
      "- rushmore\n",
      "- landforms\n",
      "\n",
      "Palabra base: christian\n",
      "Palabras más similares:\n",
      "- christianity\n",
      "- christ\n",
      "- favourably\n",
      "- supremist\n",
      "- christians\n",
      "\n",
      "Palabra base: science\n",
      "Palabras más similares:\n",
      "- cognitivists\n",
      "- behaviorists\n",
      "- scientific\n",
      "- empirical\n",
      "- sects\n"
     ]
    }
   ],
   "source": [
    "# Transponemos la matriz documento-término\n",
    "X_words = X_train.T\n",
    "\n",
    "# vocabulario: índice -> palabra\n",
    "vocab = np.array(tfidfvect.get_feature_names_out())\n",
    "\n",
    "# Elegimos manualmente 5 palabras \n",
    "chosen_words = [\"congres\", \"neighborhood\", \"mars\", \"christian\", \"science\"]\n",
    "\n",
    "for word in chosen_words:\n",
    "    if word in tfidfvect.vocabulary_:\n",
    "        idx = tfidfvect.vocabulary_[word]\n",
    "        # calculamos similaridad coseno con todas las demás palabras\n",
    "        cossim = cosine_similarity(X_words[idx], X_words)[0]\n",
    "        cossim[idx] = -1  # Ignoramos la palabra consigo misma\n",
    "        mostsim = np.argsort(cossim)[::-1][:5]\n",
    "        \n",
    "        print(f\"\\nPalabra base: {word}\")\n",
    "        print(\"Palabras más similares:\")\n",
    "        for i in mostsim:\n",
    "            print(\"-\", vocab[i])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dc013",
   "metadata": {},
   "source": [
    "Los resultados obtenidos parecen tener bastante sentido:\n",
    "\n",
    "Para la palabra “congreso” (la tuve que poner como \"congres\" y no \"congress\") surgieron cinco términos muy relacionados con la práctica política, como acción, decisión, propaganda, declaración y discusión.\n",
    "\n",
    "Para la palabra “vecindario” las asociaciones sugieren que suele aparecer en contextos de mediciones estadísticas para abordar algún problema social (como adicciones).\n",
    "\n",
    "En el caso de “Marte”, la relación es menos obvia, probablemente porque no está tan presente en muchos documentos. Pero palabras como observador y exobiólogo mantienen cierta conexión con Marte y con actividades de la NASA por ejemplo.\n",
    "\n",
    "Con la palabra “cristiano” aparecieron Cristo, cristianos y cristianismo, que son demasiado similares entre sí (algo que quizá podría resolverse con stemming). Además, surgieron favorable y supremacista, términos que suelen vincularse al debate político en torno a los cristianos en Estados Unidos.\n",
    "\n",
    "Por último, para la palabra “ciencia” aparecieron términos muy cercanos, propios de distintas prácticas científicas. Llama la atención la aparición de sectas, lo que sugiere que en este corpus también se contrasta con frecuencia la diferencia entre ambas categorías."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prueba2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
